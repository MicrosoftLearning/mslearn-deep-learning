{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Copy the flight data to the default datastore\r\n",
        "\r\n",
        "In this notebook, we will be using a series of shell scripts to download the high-dimensional airline data: the Airline Service Quality Performance dataset, distributed by the U.S. Bureau of Transportation Statistics. 1987-2021. https://www.bts.dot.gov/browse-statistical-products-and-data/bts-publications/airline-service-quality-performance-234-time)\r\n",
        "\r\n",
        "This dataset is open source and provided on an ongoing basis by the U.S. Bureau of Transportation Statistics.\r\n",
        "\r\n",
        "Each month, the Bureau publishes a new csv file containing all flight information for the prior month. A single month of data may not be enough to build out a robust machine learning model. Our goal is to download several years' worth of this data and then combine it into a single csv file.\r\n",
        "\r\n",
        "In addition to the flight data, we will also be downloading a file containing metadata and geo-coordinates of each airport and a file containing the code mappings for each airline. Airlines and airports rarely change, and as such, these files are static and do not change on a monthly basis. They do, however, contain information that we will later need to be mapped to the full airline dataset. (Megginson, David. \"airports.csv\", distributed by OurAirports. August 2, 2021. https://ourairports.com/data/airports.csv)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\r\n",
        "\r\n",
        "For easy reuse, we want to copy the data to the default datastore which is an Azure Storage Account. To get the data, we start by copying the data from the source to the compute instance. Then, we'll upload it to the datastore."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the location for data and name for final raw csv\r\n",
        "data_dir = \"./data\"\r\n",
        "csvfile_full = f\"{data_dir}/airlines_raw_data_full.csv\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1636483081999
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\r\n",
        "\r\n",
        "# Files are stored on bts.gov with 1 file per-month, download each month for a subset of years\r\n",
        "for month in `seq 1 8`; do \r\n",
        "  for year in `seq 2019 2019`; do\r\n",
        "    wget -q --no-check-certificate https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_${year}_${month}.zip\r\n",
        "    unzip -qou On_Time_Reporting_Carrier_On_Time_Performance_1987_present_${year}_${month}.zip\r\n",
        "  done\r\n",
        "done"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$data_dir\"\r\n",
        "data_dir=\"${1}\"\r\n",
        "\r\n",
        "# Remove the temporary .zip files and combine all CSV files into a single large file\r\n",
        "rm -f On_Time_Reporting_Carrier_On_Time_Performance_1987_present_*zip*\r\n",
        "mkdir -p \"${data_dir}\"\r\n",
        "cat *csv > \"${data_dir}/airlines_raw_data_full.csv\"\r\n",
        "rm On_Time_Reporting_Carrier_On_Time_Performance*csv"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$data_dir\"\r\n",
        "data_dir=\"${1}\"\r\n",
        "\r\n",
        "# Download some static files that describe metadata about airports, airlines, and locations\r\n",
        "cd \"${data_dir}\"\r\n",
        "wget -q --no-check-certificate https://ourairports.com/data/airports.csv\r\n",
        "wget -q --no-check-certificate  https://sagemaker-rapids-hpo-us-west-2.s3-us-west-2.amazonaws.com/airline_csv/carriers.csv"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the data\r\n",
        "\r\n",
        "Let's check whether we have all the data, how much data we have and what it contains."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$data_dir\"\r\n",
        "data_dir=\"${1}\"\r\n",
        "\r\n",
        "# Print out some debug information about the downloaded files\r\n",
        "ls -lh \"${data_dir}/airlines_raw_data_full.csv\"\r\n",
        "ls -lh \"${data_dir}/carriers.csv\"\r\n",
        "ls -lh \"${data_dir}/airports.csv\"\r\n",
        "wc -l  \"${data_dir}/airlines_raw_data_full.csv\"\r\n",
        "wc -l  \"${data_dir}/carriers.csv\"\r\n",
        "wc -l  \"${data_dir}/airports.csv\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$data_dir\"\r\n",
        "data_dir=\"${1}\"\r\n",
        "\r\n",
        "# Take a look at the headers for each of the CSV files\r\n",
        "head -n 1 \"${data_dir}/airlines_raw_data_full.csv\"\r\n",
        "echo\r\n",
        "head -n 1 \"${data_dir}/carriers.csv\"\r\n",
        "echo\r\n",
        "head -n 1 \"${data_dir}/airports.csv\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload all the files to the default datastore\r\n",
        "\r\n",
        "To process the data with cuDF we need to use the compute cluster. To give the cluster access to our data, we will upload all the files to the default datastore (the Azure Storage Account created together with the Azure Machine Learning workspace)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "ws = Workspace.from_config()\r\n",
        "# Get the default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "default_ds.upload_files(files=['./data/airlines_raw_data_full.csv', './data/airports.csv', './data/carriers.csv'], # Upload the diabetes csv files in /data\r\n",
        "                       target_path='airport-data/', # Put it in a folder path in the datastore\r\n",
        "                       overwrite=True, # Replace existing files of the same name\r\n",
        "                       show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1636486155034
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check whether the data has successfully been uploaded by going to [https://portal.azure.com](https://portal.azure.com). Navigate to the Azure Storage Account created with the Azure Machine Learning workspace (both will be in the same resource group).\r\n",
        "\r\n",
        "In the Storage Account, go to containers. In the container starting with the prefix `azureml-blobstore` and find the `airport-data` folder."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
